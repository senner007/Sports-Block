{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import OrderedDict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data/labels length:  8128 8128\n",
      "Validation data/labels length:  338 338\n"
     ]
    }
   ],
   "source": [
    "from csv_data import get_vocab_dict\n",
    "from csv_data import get_sports\n",
    "from csv_data import csv_to_list\n",
    "from csv_data import split_data\n",
    "from csv_data import extract_data\n",
    "from csv_data import check_duplicates\n",
    "\n",
    "\n",
    "# # TODO : fjern ord der er kategorisert som \"egennavn\" i ddo_fullforms_2020-08-26.csv\n",
    "\n",
    "ordered_dict = get_vocab_dict()\n",
    "df_sport = get_sports()\n",
    "nationalities = csv_to_list('resources/nat2.csv')\n",
    "danske_navne = csv_to_list('resources/danmark_navne.csv')\n",
    "andre_navne = csv_to_list('resources/andre_navne.csv')\n",
    "countries = csv_to_list('resources/countries.csv')\n",
    "navne = csv_to_list('resources/navne.csv')\n",
    "\n",
    "def get_results_in_data(train_data, train_labels):\n",
    "    results = []\n",
    "    for t in range(len(train_data)):\n",
    "        if train_labels[t] == 1:\n",
    "            results.append(train_data[t])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# train_data_results = get_results_in_data(train_data, train_labels)\n",
    "\n",
    "data_total, labels_total = extract_data(df_sport)\n",
    "check_duplicates(data_total)\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = split_data((data_total, labels_total), 4)\n",
    "\n",
    "# print(\"Total data: \", len(train_text))\n",
    "print(\"Train data/labels length: \", len(train_data), len(train_labels))\n",
    "print(\"Validation data/labels length: \", len(val_data),  len(val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KORT SPORT . Anne-Marie Rindoms træner vinder pris . Bag de fleste store sportslige resultater findes en træner, der hjælper med at få præstationerne at gå op i en højere enhed\n",
      "0\n",
      "Fodbold . Højlund nomineret til fornem pris . Den danske landsholdsspiller er én af ti spillere, der kan vinde prisen som den bedste spiller under 21 år\n",
      "0\n",
      "KORT SPORT . Fodboldlandsholdet vinder fair play-pris . Herrelandsholdet i fodbold modtager prisen Special World Fair Play Trophy, som bliver uddelt af Den Internationale Fair Play Komité\n",
      "0\n",
      "Formel 1 . Magnussen vinder fornem pris . Kevin Magnussen deler nu pris med blandt andre Valtteri Bottas, Daniel Ricciardo og Lewis Hamilton\n",
      "0\n",
      "Tour de France . Holdkammerat rørt over Asgreens hilsen . Dries Devenyns sætter stor pris på danskerens ord\n",
      "0\n",
      "Boksning . Kessler: - Hvis der er det mindste, bokser jeg ikke . Mikkel Kessler vil ikke i bokseringen for enhver pris. Helbredet skal være fuldstændig i orden\n",
      "0\n",
      "Fodbold . O'Riley hædret med fornem pris . Den danske midtbanespiller blev i oktober udtaget til A-landsholdet for første gang\n",
      "0\n",
      "SPORT . AFSTEMNING Hvem skal vinde priserne til Sport 2021? . DR Sportens årlige sportsshow bliver sendt fredag klokken 21.30, og i artiklen her kan du se de nominerede til priserne\n",
      "0\n",
      "OL . Danmark vil gerne vinde gruppen, men ikke for enhver pris . Danmark kan tåle at tabe med fire til Sverige i den sidste gruppekamp og stadig slutte på førstepladsen\n",
      "0\n",
      "SPORT . JP-journalist vinder fornem pris . Jyllands-Postens journalist Dan Philipsen blev onsdag kåret som Årets Sportsjournalist 2009\n",
      "0\n",
      "SPORT . Sparta Prag lægger to år på kontrakten med Brian Priske . Den tjekkiske klub Sparta Prag har forlænget aftalen med cheftræner Brian Priske, så den nu gælder til 2026\n",
      "0\n",
      "Amerikansk fodbold . - Helt vildt, at I er fløjet hele vejen fra Danmark, siger NFL-stjerne . Haason Reddick og Jason Kelce sætter pris på interessen for NFL globalt\n",
      "0\n",
      "Sejlsport . Rindom nomineret til stor pris: - Den største anerkendelse . Anne-Marie Rindom kan for anden gang i træk blive modtager af en fornem pris fra sejlsportens verdensforbund\n",
      "0\n",
      "Ishockey VM . Dansk profil smadrede glas i vrede . Markus Lauridsen lod følelserne få frit løb, og det betalte en glasrude prisen for\n",
      "0\n",
      "Fodbold . Priske forlænger aftalen med Sparta Prag . Den tjekkiske klub Sparta Prag har forlænget aftalen med cheftræner Brian Priske, så den nu gælder til 2026\n",
      "0\n",
      "Golf . Helligkilde vinder Den Gyldne Golfbold . Golfspilleren Marcus Helligkilde vinder prisen Den Gyldne Golfbold for sine præstationer på Challenge Tour\n",
      "0\n",
      "Basketball . NBA-hold solgt for milliardbeløb i historisk handel . Phoenix Suns skifter ejer i den største handel i NBA-historien. Prisen er på 28 milliarder kroner\n",
      "0\n",
      "KORT SPORT . Årets hold: Dansk baskettalent vinder endnu en pris i USA . Alberte Rimdal har været rigtig god med en basketball i år\n",
      "0\n",
      "Formel 1 . Magnussen hædret med pris . Den danske Formel 1-kører lavede en imponerende overhaling i Monacos Grand Prix\n",
      "0\n",
      "Fodbold . Eksperterne er ikke et sekund i tvivl om, hvem der vinder Ballon d'Or . Messi har syv gange vundet den fornemme pris. Det har ingen anden præsteret\n",
      "0\n",
      "SPORT . Ulykke satte Anders Linds liv i perspektiv: 'Jeg er i live. Det er det vigtigste' . Den danske bordtennisspiller brækkede to ryghvirvler efter et trafikuheld for syv måneder siden. Det har lært ham at sætte pris på dagligdagen\n",
      "0\n",
      "Fodbold . Messi nomineret til pris - har kun spillet fire kampe . Prisen som årets spiller i den amerikanske MLS kan gå til den argentinske superstjerne\n",
      "0\n",
      "KORT SPORT . Simon Kjær og lægestab modtager pris fra Uefas præsident . Simon Kjær og den danske lægestab, der var til stede under EM-kampen mod Finland, da Christian Eriksen faldt om med et hjertestop, har modtaget en pris fra Det Europæiske Fodboldforbund, Uefa\n",
      "0\n",
      "KORT SPORT . Svensk atletikstjerne vinder stor pris . Den svenske stangspringer Armand Duplantis vandt mandag aften prisen som årets bedste mandlige atletikudøver i verden\n",
      "0\n",
      "SPORT . OVERBLIK Her er alle prisvinderne ved Sport 2018 . 13 priser blev lørdag delt ud ved DR's store Sport 2018-show. Se, hvem der vandt\n",
      "0\n",
      "Fodbold . Dansker scorer pris for sin klimaindsats . Klimaforkæmperen Sofie Junge Pedersen er udpeget som modtager af Marcus Rashford Award\n",
      "0\n",
      "Basketball . Ikonisk Bryant-trøje solgt for rekordbeløb . Prisen er rekord for genstande, der har tilhørt basketspilleren, som omkom i en ulykke for tre år siden\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for i,tt in enumerate(data_total):\n",
    "    if re.search(r'\\bpris', tt, re.IGNORECASE):\n",
    "        if (labels_total[i] == 0):\n",
    "            print(tt)\n",
    "            print(labels_total[i])\n",
    "        # print(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4545 3583]\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(train_labels)\n",
    "# print(\n",
    "#     \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "#         counts[1], 100 * float(counts[1]) / len(train_labels)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "print(counts)\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_articles_to_csv():\n",
    "    df_sport_combined = df_sport.copy().drop('Link', axis=1)\n",
    "    df_sport_combined.to_csv('articles_temp/combined.csv')\n",
    "\n",
    "combine_articles_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in data points: \n",
      "Empty DataFrame\n",
      "Columns: [Category, Headline, SubHeading, Link, isResult, isMaybe]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df_sport.duplicated()\n",
    "print(\"Duplicates in data points: \")\n",
    "print(df_sport[duplicate_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "isin_dict = False\n",
    "def test_lookup_performance():\n",
    "    word_to_check = \"Dansk\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x in range(1000000):\n",
    "        isin_dict = word_to_check in ordered_dict\n",
    "\n",
    "    end_time = time.time()  \n",
    "    assert(end_time - start_time < 1)\n",
    "    print(isin_dict)\n",
    "\n",
    "test_lookup_performance()\n",
    "\n",
    "# isin_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mads', 'real', 'trine', 'rune', 'o', 'norman', 'søren', 'tiger', 'signe', 'kim', 'mark', 'groves']\n",
      "total unique words: 9040\n",
      "total sports lingo words: 2177\n",
      "total vocab: 6863\n",
      "total articles: 8466\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train_text = df_sport.iloc[:, [0,1,2]].apply(' . '.join, axis=1).replace('\\xa0', '', regex=True).to_numpy()\n",
    "# train_text_results = df_sport.loc[df_sport['isResult'] == True].iloc[:, [0,1,2]].apply(' . '.join, axis=1).replace('\\xa0', '', regex=True).to_numpy()\n",
    "\n",
    "from create_vocab import split_sentences\n",
    "from create_vocab import remove_duplicates\n",
    "from create_vocab import remove_nationalities\n",
    "from create_vocab import remove_danske_navne\n",
    "from create_vocab import remove_danske_fornavne\n",
    "from create_vocab import remove_numeric\n",
    "from create_vocab import remove_non_dict_words\n",
    "from create_vocab import add_non_alpha_numeric\n",
    "\n",
    "# words_total =  train_data.copy()\n",
    "# words_total.extend(val_data)\n",
    "\n",
    "def words_by_frequency(arr):\n",
    "    xx = np.array(arr, dtype=object)\n",
    "    unique, counts = np.unique(xx, return_counts=True)\n",
    "    aa = np.asarray((unique, counts)).T\n",
    "    return np.flip(aa[aa[:, 1].argsort()])\n",
    "\n",
    "def remove_non_frequent(words_arr, threshold):\n",
    "    words_dict = words_by_frequency(words_arr)\n",
    "    words_above_threshold = []\n",
    "    for f in words_dict:\n",
    "        if f[0] > threshold:\n",
    "            words_above_threshold.append(f[1])\n",
    "    return words_above_threshold\n",
    "\n",
    "\n",
    "sentence_words = split_sentences(train_data)\n",
    "sentence_words_frequent = remove_non_frequent(sentence_words, 1)\n",
    "words_arr_unique = remove_duplicates(sentence_words_frequent)\n",
    "words_arr_unique = remove_nationalities(words_arr_unique, nationalities)\n",
    "words_arr_unique = remove_danske_navne(words_arr_unique, danske_navne)\n",
    "words_arr_unique = remove_danske_fornavne(words_arr_unique, andre_navne)\n",
    "words_arr_unique = remove_numeric(words_arr_unique)\n",
    "\n",
    "words_train_vocab, words_sport_lingo = remove_non_dict_words(words_arr_unique, ordered_dict)\n",
    "\n",
    "\n",
    "\n",
    "# # TODO : brug tensorflow Tokenezier til at omdanne ord til tokens\n",
    "# # TODO : søg i alle leksikoner, søg med og uden bindestreg\n",
    "# # TODO : håndter tal ikke i ordbøger eks ( x-x eller x-årig)\n",
    "# # TODO : lemmatizer : udelad bøjninger af samme navneord. eks : verdensmester/verdensmesteren\n",
    "# # TODO : evt. grupper ord der ofte hænger sammen med nltk BigramFinder. eks vandt over\n",
    "# TODO : fjern evt. også alle navne (fornavne og efternavne)  \n",
    "\n",
    "print(\"total unique words:\", len(words_arr_unique) )\n",
    "print(\"total sports lingo words:\", len(words_sport_lingo) )\n",
    "print(\"total vocab:\", len(words_train_vocab))\n",
    "print(\"total articles:\", len(df_sport) )\n",
    "\n",
    "# for d in df_sport['isResult']:\n",
    "#     if isinstance(d, bool) != True:\n",
    "#         print(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6863"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(words_train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('words_sport_lingo.txt','w')\n",
    "for item in words_sport_lingo:\n",
    "\tfile.write(item+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('words_train_vocab.txt','w')\n",
    "for item in sorted(words_train_vocab):\n",
    "\tfile.write(item+\"\\n\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diaz\n",
      "rathore\n",
      "ragnhild\n",
      "meyerhoff\n",
      "barracks\n",
      "aage\n",
      "fikri\n",
      "eto\n",
      "asbjørn\n",
      "emmons\n",
      "goldings\n",
      "mowinckel\n",
      "samuel\n",
      "lahoz\n",
      "thuesen\n",
      "ke\n",
      "grimmel\n",
      "mateu\n",
      "torben\n",
      "spejlsgaard\n",
      "sico\n",
      "goncalo\n",
      "skeet\n",
      "grundsøe\n",
      "wahl\n",
      "golding\n",
      "kazan\n",
      "maulana\n",
      "tite\n",
      "schip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TODO : lav en negativ liste også\n",
    "# display most frequent words found in lingo words\n",
    "for f in remove_duplicates(sentence_words):\n",
    "    if f in words_sport_lingo and f not in navne and len(f) > 1:\n",
    "        print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO : lav en negativ liste også\n",
    "# display most frequent words found in lingo words\n",
    "# words_that_appear_once = []\n",
    "# for f in frequent_words:\n",
    "#     if f[0] < 2 and f[1] not in navne:\n",
    "#         words_that_appear_once.append(f[1])\n",
    "\n",
    "# # print(len(words_that_appear_once))\n",
    "# sorted(words_that_appear_once)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# f = frequent_words[-300:]\n",
    "# ff = []\n",
    "# for w in frequent_words:\n",
    "#     if w[1] in words_train_vocab and w[0] < 2:\n",
    "#         ff.append(w[1])\n",
    "\n",
    "# print(len(ff))\n",
    "# ff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab/max_features :  6808\n",
      "['tidligere' 'formel' 'xnumber' 'kører' 'vinder' '[UNK]' 'guld' 'den'\n",
      " 'tidligere' 'formel' 'xnumber' 'kører' '[UNK]' '[UNK]' 'kørte' 'galt' 'i'\n",
      " 'xyear' 'og' 'mistede' 'begge' 'sine' 'ben' '.' 'i' 'dag' 'vandt' 'han'\n",
      " 'guld' 'ved' '[UNK]' 'i' 'håndcykel' '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from vectorization import replace_finals, replace_nationality, to_lower\n",
    "from vectorization import split_dash\n",
    "from vectorization import split_included_specials\n",
    "from vectorization import replace_tournament\n",
    "from vectorization import replace_countries\n",
    "from vectorization import replace_weekday\n",
    "from vectorization import replace_digits\n",
    "from vectorization import vect_layer_2_text\n",
    "from vectorization import vectorize_layer\n",
    "from vectorization import standardize\n",
    "from static_data import tournaments\n",
    "from static_data import weekdays\n",
    "from static_data import non_alpha\n",
    "from static_data import word_generalization\n",
    "\n",
    "\n",
    "# TODO : evt indikere hvilke navneord der starte med stort bogstav(egenavne), evt. lave et opslag for at undersøge ordklasse for det første ord i sætningen \n",
    "# TODO : test hvilke standarization funktioner giver bedre resultater \n",
    "\n",
    "arrs = [\n",
    "    to_lower, \n",
    "    split_dash, \n",
    "    split_included_specials, \n",
    "    replace_tournament(tournaments),\n",
    "    replace_countries(countries), \n",
    "    replace_weekday(weekdays), \n",
    "    replace_finals,\n",
    "    replace_nationality(nationalities),\n",
    "    replace_digits\n",
    "]\n",
    "\n",
    "s = standardize(arrs)\n",
    "\n",
    "words_train_vocab.extend(word_generalization)\n",
    "words_train_vocab.extend(non_alpha)\n",
    "\n",
    "# Model constants.\n",
    "max_features = 6900\n",
    "sequence_length = 60\n",
    "\n",
    "vectorized_layer = vectorize_layer(max_features, sequence_length, s)\n",
    "\n",
    "text_ds = vectorized_layer.adapt(words_train_vocab)\n",
    "vect_vocab = vectorized_layer.get_vocabulary()\n",
    "\n",
    "print(\"Total vocab/max_features : \",  len(vect_vocab))\n",
    "\n",
    "print (vect_layer_2_text(vectorized_layer([\"Tidligere Formel 1-kører vinder PL-guld ,Den tidligere Formel 1-kører Alex Zanardi kørte galt i 2001 og mistede begge sine ben. I dag vandt han guld ved PL i håndcykel.\"]), vect_vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in train_data[0:50]:\n",
    "#     print(\"Original \\n:\", t)\n",
    "#     print(\"Text from vectorized: \\n\", vect_layer_2_text(\n",
    "#         vectorized_layer([t]), vect_vocab\n",
    "#         ))\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vect = vectorized_layer(train_data)\n",
    "val_data_vect = vectorized_layer(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.4):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy(threshold=0.97)\n",
    "m.update_state([[1], [1], [0], [0]], [[0.98], [0.98], [0], [0]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import random as python_random\n",
    "\n",
    "def get_cnn_model():\n",
    "\n",
    "    embedding_dim = 24\n",
    "\n",
    "    # A integer input for vocab indices.\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "    # 'embedding_dim'.\n",
    "    x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    # x = layers.Conv1D(32, 11, padding=\"valid\", activation=\"relu\")(x)\n",
    "    # x = layers.Conv1D(128, 9, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(32, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    # x = layers.Conv1D(32, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    # x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    # x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    \n",
    "    cnn_model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return cnn_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import random as python_random\n",
    "\n",
    "\n",
    "def get_transformer_model():\n",
    "\n",
    "    embed_dim =  192 # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 192  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "    # A integer input for vocab indices.\n",
    "    inputs = tf.keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "    # 'embedding_dim'.\n",
    "    # x = layers.Embedding(max_features, embed_dim)(inputs)\n",
    "\n",
    "    embedding_layer = TokenAndPositionEmbedding(sequence_length, max_features, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # x = transformer_block(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    # x = layers.Conv1D(128, 10, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    # x = layers.Conv1D(128, 10, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    # x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "\n",
    "    transformer_model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    bn =   tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.97)\n",
    "    \n",
    "    \n",
    "    transformer_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "    return transformer_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_model(name):\n",
    "    if (name == \"cnn\"):\n",
    "       return get_cnn_model()\n",
    "    elif (name == \"transformer\"):\n",
    "       return get_transformer_model()\n",
    "  \n",
    "\n",
    "def filter_max_accuracy(history, threshold = 0.95):\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    list = []\n",
    "    for x in range(len(acc)):\n",
    "        if (acc[x] > threshold):\n",
    "            list.append(val_acc[x])\n",
    "\n",
    "    return np.array(list)\n",
    "\n",
    "models = [\"cnn\", \"transformer\"]\n",
    "\n",
    "callback_3_loss = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4)\n",
    "\n",
    "\n",
    "def mean_model_accuracy(mode_names, iterations, epochs = 20):\n",
    "\n",
    "  \n",
    "    results = []\n",
    "\n",
    "    for name in range(len(mode_names)):\n",
    "        model_name = mode_names[name]\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for x in range(iterations):\n",
    "            model = prepare_model(model_name)\n",
    "\n",
    "            # Fit the model using the train and test datasets.\n",
    "            history = model.fit(train_data_vect, train_labels, epochs=epochs, batch_size=6, validation_data=(val_data_vect, val_labels), callbacks=[callback_3_loss])\n",
    "\n",
    "            max_val_acc = filter_max_accuracy(history)\n",
    "            val_accuracies.append(max(max_val_acc))\n",
    "            print(max(max_val_acc))\n",
    "            print(val_accuracies)\n",
    "        \n",
    "        d = dict(name = model_name, results = np.mean(np.squeeze(np.array(val_accuracies))))\n",
    "        results.append(d)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_results = mean_model_accuracy(models, 8)\n",
    "# mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_format_round(result):\n",
    "    return round(result)\n",
    "\n",
    "def result_format_none(result):\n",
    "    return result\n",
    "\n",
    "def print_model_score(model):\n",
    "    score = model.evaluate(val_data_vect, val_labels, verbose=0)\n",
    "    print(\"Validation loss:\", score[0])\n",
    "    print(\"Validations accuracy:\", score[1])\n",
    "\n",
    "def print_validation_results(predictions, val_data, labels, formatter, only_incorrect = False):\n",
    "    print(\"Number of predictions\", len(predictions))\n",
    "    n_correct = 0\n",
    "    for x in range(len(val_data)):\n",
    "        correct_prediction = result_format_round(labels[x]) == result_format_round(predictions[x][0])\n",
    "        if correct_prediction:\n",
    "            n_correct += 1\n",
    "\n",
    "        if correct_prediction == False and labels[x] == 0:\n",
    "            print(\"VALIDATION SAMPLE TEXT: \\n\" ,val_data[x])\n",
    "            print(\"VALIDATION SAMPLE DE-VECTORIZED: \\n\" ,vect_layer_2_text(val_data_vect[x], vect_vocab))\n",
    "            print(\"LABEL -- :\" , labels[x])\n",
    "            print(\"PREDICTION -- :\" , formatter(predictions[x][0]), \" ---- float: \", predictions[x][0])\n",
    "            print(\"CORRECT PREDICTION: \", correct_prediction)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    print(\"Number correct: \", n_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "136/136 [==============================] - 3s 18ms/step - loss: 1.3949e-04 - accuracy: 0.7141 - val_loss: 0.3741 - val_accuracy: 0.8254\n",
      "Epoch 2/6\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.8561e-05 - accuracy: 0.8901 - val_loss: 0.2830 - val_accuracy: 0.8846\n",
      "Epoch 3/6\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 3.5580e-05 - accuracy: 0.9524 - val_loss: 0.2649 - val_accuracy: 0.8905\n",
      "Epoch 4/6\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 1.8475e-05 - accuracy: 0.9813 - val_loss: 0.2470 - val_accuracy: 0.8876\n",
      "Epoch 5/6\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 8.8727e-06 - accuracy: 0.9953 - val_loss: 0.2509 - val_accuracy: 0.8905\n",
      "Epoch 6/6\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 5.1515e-06 - accuracy: 0.9984 - val_loss: 0.2391 - val_accuracy: 0.8964\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs= 6\n",
    "transformer_model = get_transformer_model()\n",
    "\n",
    "transformer_history = transformer_model.fit(train_data_vect, train_labels, epochs=epochs, batch_size=60, validation_data=(val_data_vect, val_labels),  class_weight=class_weight,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 60)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 60, 192)           1336320   \n",
      "_________________________________________________________________\n",
      "transformer_block_2 (Transfo (None, 60, 192)           371136    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 193       \n",
      "=================================================================\n",
      "Total params: 1,707,649\n",
      "Trainable params: 1,707,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epochs= 8\n",
    "# cnn_model = get_cnn_model()\n",
    "\n",
    "# transformer_history = cnn_model.fit(train_data_vect, train_labels, epochs=epochs, batch_size=40, validation_data=(val_data_vect, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model):\n",
    "    np.set_printoptions(precision = 5, suppress = True)\n",
    "    predictions = model.predict(val_data_vect)\n",
    "    print_model_score(model)\n",
    "    print(\"\\n\")\n",
    "    print_validation_results(predictions, val_data, val_labels, result_format_round)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRANSFORMER ---\n",
      "Validation loss: 0.2391483336687088\n",
      "Validations accuracy: 0.8964496850967407\n",
      "\n",
      "\n",
      "Number of predictions 338\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Fodbold . Så vild er forskellen på Danmark og San Marino . San Marino har vundet én fodboldkamp i nationens historie\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['fodbold' '.' 'så' 'vild' 'er' 'forskellen' 'på' 'xland' 'og' 'xland' '.'\n",
      " 'xland' 'har' 'vundet' '[UNK]' 'fodboldkamp' 'i' 'nationens' 'historie']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.918396\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Fodbold . United-spiller tog initiativ til at afbryde svensk landskamp . Victor Lindelöf trådte i karakter\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['fodbold' '.' '[UNK]' 'spiller' 'tog' '[UNK]' 'til' 'at' 'afbryde'\n",
      " 'xnationality' 'landskamp' '.' '[UNK]' '[UNK]' '[UNK]' 'f' 'trådte' 'i'\n",
      " '[UNK]']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.50244087\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Vinter-OL . Dopingmistænkt kunstskøjteløber må fortsætte ved OL . Kamila Valieva er fortsat dopingmistænkt, men må konkurrere videre ved vinter-OL i Beijing\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['vinter' 'xtournament' '.' 'dopingmistænkt' 'kunstskøjteløber' 'må'\n",
      " 'fortsætte' 'ved' 'xtournament' '.' '[UNK]' '[UNK]' 'er' 'fortsat'\n",
      " 'dopingmistænkt' 'men' 'må' 'konkurrere' 'videre' 'ved' 'vinter'\n",
      " 'xtournament' 'i' '[UNK]']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.5191529\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Badminton . Melder afbud i denne uge efter stor triumf . Efter tre uger i træk med store bedrifter skal Kim Astrup og Anders Skaarup hjem og lade batterierne op\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['badminton' '.' 'melder' 'afbud' 'i' 'denne' 'uge' 'efter' 'stor'\n",
      " 'triumf' '.' 'efter' 'tre' 'uger' 'i' 'træk' 'med' 'store' '[UNK]' 'skal'\n",
      " '[UNK]' '[UNK]' 'og' '[UNK]' '[UNK]' 'hjem' 'og' 'lade' '[UNK]' 'op']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.88392264\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " SPORT . OVERBLIK Her er alle prisvinderne ved Sport 2018 . 13 priser blev lørdag delt ud ved DR's store Sport 2018-show. Se, hvem der vandt\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['sport' '.' 'overblik' 'her' 'er' 'alle' '[UNK]' 'ved' 'sport' 'xyear'\n",
      " '.' 'xnumber' '[UNK]' 'blev' 'xweekday' 'delt' 'ud' 'ved' '[UNK]' 's'\n",
      " 'store' 'sport' 'xyear' 'show' '.' 'se' 'hvem' 'der' 'vandt']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.69043356\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Amerikansk fodbold . Ekspert glæder sig til NFL-brag: - Det bliver meget dramatisk og vildt . De to hold med flest scorede point mødes søndag aften, og der er lagt op til spænding til det sidste\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['xnationality' 'fodbold' '.' 'ekspert' 'glæder' 'sig' 'til' 'xtournament'\n",
      " 'brag' ':' 'det' 'bliver' 'meget' 'dramatisk' 'og' 'vildt' '.' 'de' 'to'\n",
      " 'hold' 'med' 'flest' 'scorede' 'point' 'mødes' 'xweekday' 'aften' 'og'\n",
      " 'der' 'er' 'lagt' 'op' 'til' 'spænding' 'til' 'det' 'sidste']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.8754192\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Fodbold . Kjær kiggede ydmygt ned, da Schmeichel fremhævede én særlig egenskab . TV 2 Sport har mødt Simon Kjær og Peter Schmeichel til et stort dobbeltinterview\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['fodbold' '.' '[UNK]' '[UNK]' '[UNK]' 'ned' 'da' '[UNK]' '[UNK]' '[UNK]'\n",
      " 'særlig' '[UNK]' '.' 'tv' 'xnumber' 'sport' 'har' 'mødt' '[UNK]' '[UNK]'\n",
      " 'og' '[UNK]' '[UNK]' 'til' 'et' 'stort' '[UNK]']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.5218172\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " CHAMPIONS LEAGUE . Stokholm tror på samlet sejr over Zenit . Trods to pauvre Superligakampe tror FC Nordsjælland-anføreren på avancement mod Zenit\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['xtournament' '.' '[UNK]' 'tror' 'på' 'samlet' 'sejr' 'over' '[UNK]' '.'\n",
      " 'trods' 'to' '[UNK]' 'superligakampe' 'tror' '[UNK]' '[UNK]' '[UNK]' 'på'\n",
      " 'avancement' 'mod' '[UNK]']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.9740967\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Formel 1 . - Det er meget sjældent, det sker, siger henrykt Magnussen . Den danske Formel 1-kører Kevin Magnussen er meget begejstret over de tre testdage, der forløb perfekt\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['formel' 'xnumber' '.' 'det' 'er' 'meget' 'sjældent' 'det' 'sker' 'siger'\n",
      " '[UNK]' '[UNK]' '.' 'den' 'xnationality' 'formel' 'xnumber' 'kører'\n",
      " '[UNK]' '[UNK]' 'er' 'meget' 'begejstret' 'over' 'de' 'tre' '[UNK]' 'der'\n",
      " 'forløb' 'perfekt']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.5926329\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Vinter-OL . Fik norsk profil til at styrte: - Er meget ked af det . Norge blev tidligt sat tilbage i langrendsstafetten ved OL, da Tiril Udnes Weng styrtede efter en kollision med en lettisk skiløber\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['vinter' 'xtournament' '.' 'fik' 'xnationality' 'profil' 'til' 'at'\n",
      " 'styrte' ':' 'er' 'meget' 'ked' 'af' 'det' '.' 'xland' 'blev' 'tidligt'\n",
      " 'sat' 'tilbage' 'i' '[UNK]' 'ved' 'xtournament' 'da' '[UNK]' '[UNK]'\n",
      " '[UNK]' 'styrtede' 'efter' 'en' '[UNK]' 'med' 'en' 'xnationality'\n",
      " 'skiløber']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.537915\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " HERRELANDSHOLDET . OVERBLIK Indviklet VM-plads, pokal-prestige og økonomiske fordele: Det betyder Nations League-triumf for Danmark . Danmark kan med en sejr over Belgien kvalificere sig til Nations League-slutspillet\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['herrelandsholdet' '.' 'overblik' '[UNK]' 'xtournament' 'plads' 'pokal'\n",
      " '[UNK]' 'og' 'økonomiske' 'fordele' ':' 'det' 'betyder' 'xtournament'\n",
      " 'triumf' 'for' 'xland' '.' 'xland' 'kan' 'med' 'en' 'sejr' 'over' 'xland'\n",
      " 'kvalificere' 'sig' 'til' 'xtournament' 'slutspillet']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.99718153\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Atletik . Han skrev løbehistorie som 26-årig, men ud af det blå kom beskeden, der ændrede alt . Henrik Jørgensen er gået bort. Karrieren toppede, da han 26-årig vandt London Marathon, men kort efter stoppede han\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['atletik' '.' 'han' 'skrev' '[UNK]' 'som' 'xnumber' 'årig' 'men' 'ud'\n",
      " 'af' 'det' 'blå' 'kom' 'beskeden' 'der' 'ændrede' 'alt' '.' '[UNK]'\n",
      " '[UNK]' 'er' 'gået' 'bort' '.' 'karrieren' '[UNK]' 'da' 'han' 'xnumber'\n",
      " 'årig' 'vandt' '[UNK]' 'marathon' 'men' 'kort' 'efter' 'stoppede' 'han']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.5219601\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " KVINDELANDSHOLDET . Danmark er semifinaleklar, og nu skal tre genialiteter sikre dem gruppesejren . Den tidligere norske landsholdsspiller Marit Malm Frafjord udpeger tre nøgler til dansk sejr mod Norge\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['kvindelandsholdet' '.' 'xland' 'er' 'xfinaleklar' 'og' 'nu' 'skal' 'tre'\n",
      " '[UNK]' 'sikre' 'dem' '[UNK]' '.' 'den' 'tidligere' 'xnationality'\n",
      " 'landsholdsspiller' '[UNK]' '[UNK]' '[UNK]' 'udpeger' 'tre' 'nøgler'\n",
      " 'til' 'xnationality' 'sejr' 'mod' 'xland']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.8442935\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "VALIDATION SAMPLE TEXT: \n",
      " Formel 1 . - Det er et sindssygt løb, siger Magnussen . Kevin Magnussen ser frem til weekendens grandprix i Japan, hvor han går benhårdt efter at score point\n",
      "VALIDATION SAMPLE DE-VECTORIZED: \n",
      " ['formel' 'xnumber' '.' 'det' 'er' 'et' 'sindssygt' 'løb' 'siger' '[UNK]'\n",
      " '.' '[UNK]' '[UNK]' 'ser' 'frem' 'til' 'weekendens' 'xtournament' 'i'\n",
      " 'xland' 'hvor' 'han' 'går' 'benhårdt' 'efter' 'at' 'score' 'point']\n",
      "LABEL -- : 0\n",
      "PREDICTION -- : 1  ---- float:  0.9183917\n",
      "CORRECT PREDICTION:  False\n",
      "\n",
      "\n",
      "Number correct:  303\n"
     ]
    }
   ],
   "source": [
    "print(\"--- TRANSFORMER ---\")\n",
    "print_results(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"--- CNN ---\")\n",
    "# print_results(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01501, -0.01034, -0.01044, ..., -0.0153 ,  0.0349 , -0.05023],\n",
       "       [ 0.0342 , -0.00634,  0.0297 , ..., -0.01406,  0.10794, -0.03105],\n",
       "       [-0.02517,  0.03326,  0.05812, ...,  0.02312,  0.04185, -0.03233],\n",
       "       ...,\n",
       "       [ 0.02215,  0.02347,  0.03482, ...,  0.0108 , -0.03363,  0.02911],\n",
       "       [-0.01721,  0.00715, -0.0445 , ..., -0.02538,  0.03992,  0.02418],\n",
       "       [-0.03343,  0.02502,  0.00721, ..., -0.03439, -0.0063 , -0.01322]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "\n",
    "ll = transformer_model.layers[1]\n",
    "ll_weights = ll.get_weights()[0]\n",
    "\n",
    "# print(ll_weights.shape)\n",
    "ll_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import I/O module in python\n",
    "import io\n",
    "\n",
    "##open the text stream for vectors\n",
    "vectors = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "##open the text stream for metadata\n",
    "meta = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "##write each word and its corresponding embedding\n",
    "for index in range(1, len(vect_vocab)):\n",
    "  word = vect_vocab[index]  # flipping the key-value in word_index\n",
    "  embeddings = ll_weights[index]\n",
    "  meta.write(word + \"\\n\")\n",
    "  vectors.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "\n",
    "##close the stream\n",
    "vectors.close()\n",
    "meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import collocations\n",
    "# bigram_measures = collocations.BigramAssocMeasures()\n",
    "# finder = collocations.BigramCollocationFinder.from_words([\"New\", \"York\", \"is\", \"big\", \"New\", \"York\", \"is\", \"dirty\"])\n",
    "# finder.ngram_fd.items()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lemmy\n",
    "# # Create an instance of the standalone lemmatizer.\n",
    "# lemmatizer = lemmy.load(\"da\")\n",
    "\n",
    "# # Find lemma for the word 'akvariernes'. First argument is an empty POS tag.\n",
    "# lemmatizer.lemmatize(\"NOUN\", \"verdensetter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk as nltk\n",
    "# # from string import punctuation\n",
    "# # from nltk.corpus import stopwords\n",
    "# # nltk.download('stopwords')\n",
    "\n",
    "# # da_stopwords = stopwords.words(\"danish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorized_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = transformer_model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "[[0.95992]\n",
      " [0.99842]\n",
      " [0.99994]\n",
      " [0.99481]\n",
      " [0.89972]]\n",
      "\n",
      " NON-Results:\n",
      "[[0.0012 ]\n",
      " [0.14054]\n",
      " [0.15431]\n",
      " [0.02175]\n",
      " [0.02937]\n",
      " [0.03946]\n",
      " [0.58403]\n",
      " [0.68906]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResults:\")\n",
    "\n",
    "\n",
    "print(end_to_end_model.predict(\n",
    "    [\n",
    "      \"Fodbold . Fjerritslev vinder over Vordingborg. Træner kommenterer på historisk kamp\",\n",
    "       \"SPORT . Hun vandt bronze i mandags Roer Anne Dsane Andersen har som 24-årig vundet bronze ved OL\",\n",
    "       \"Badminton . Axelsen frustreret over nederlag. Viktor Axelsen trænger til ferie efter nedturen\",\n",
    "      \"OL . Det blev til en flot medalje til Malene dfhsds. 'Jeg er meget lykkelig for resultatet'\",\n",
    "      \"Badminton . Dansker er videre til finalerne. dsfsdf sfdsdf slkal spille i semifinalerne på onsdag\",\n",
    "     ]))\n",
    "\n",
    "\n",
    "print(\"\\n NON-Results:\") \n",
    "print(end_to_end_model.predict(\n",
    "    [\n",
    "      \"OL Meget skal ske før en medalje kommer inden for rækkevidde. Dressurrytter Malene dsds har mistet troen på success\",\n",
    "      \"Fodbold . Træner for Fjerritslev ser frem til sejr over Vordingborg. 'Det bliver en historisk kamp'\",\n",
    "      \"Fodbold . De danske spillere skal op imod Sverige, som de tabte til i 2022\",\n",
    "      \"Fodbold . De danske spillere vil forsøge at besejre Tyrkiet den kommende Lørdag i VM-kamp. Tyrkiet har aldriv været i en VM-finale\",\n",
    "      \"Fodbold . De danske spillere tror på sejr mod Tyrkiet. 'Den skal vindes'\",\n",
    "      \"Skisport . Sverige drømmer om flere medaljer og sejre til næste års OL . Træner forventer flere gode resultater\",\n",
    "      \"Boksning . Kesler vil overraske alle og gøre det umulige. 'Jeg vinder i VM'\",\n",
    "      \"Boksning . Kesler med stor selvtillid: 'Det bliver guld eller sølv til VM'\",\n",
    "    \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IN-BETWEEN-Results:\n",
      "[[0.99997]\n",
      " [0.99984]\n",
      " [0.96174]\n",
      " [0.66457]\n",
      " [0.36849]\n",
      " [0.00025]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n IN-BETWEEN-Results:\") \n",
    "print(end_to_end_model.predict(\n",
    "    [\n",
    "       \"Fodbold . Fjerritslev vandt i lørdags over Vordingborg 1-0. Den danske anfører dasdad dasdasd triumferer\",\n",
    "       \"Fodbold . Fjerritslev vandt i lørdags over Vordingborg. Den danske anfører adasdasdd daddas triumferer\",\n",
    "       \"Fodbold . Fjerritslev vandt i lørdags over Vordingborg. Efter kampen meddelyte den danske anfører sdfd sdfdf, at han skal under kniven\",\n",
    "       \"Fodbold . Superliga-profil har meddelelse efter sejr over Vordingborg. ' Den danske anfører fsdsdff sdffsd skal opereres og er ude i flere måneder\",\n",
    "       \"Fodbold . Superliga-profil har meddelelse efter sejr. Den danske anfører fdfd sdffdf skal opereres og er ude i flere måneder\",\n",
    "       \"Fodbold . Superliga-profil kan se frem til en længere pause. Den danske anfører fdfd sdfff skal opereres og er ude i flere måneder\",\n",
    "       \n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IN-BETWEEN-Results:\n",
      "[[0.0004]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n IN-BETWEEN-Results:\") \n",
    "print(end_to_end_model.predict(\n",
    "    [\n",
    "       \"FODBOLD . Fifa bekræfter: Kun Saudi Arabien har budt på VM-værtskab i 2034 . Det er kun Saudi-Arabien, der har budt sig til i kampen om at afholde VM i fodbold i 2034\"\n",
    "       \n",
    "        ]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be0503bf1d8a1ee3ca0077be831d95fbcddd9686f11808f41fa1809452b7e6ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('newenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
