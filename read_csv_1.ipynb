{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import OrderedDict\n",
    "def get_sports():\n",
    "    df_sport_latest = pd.read_csv('sports_articles.csv', encoding = \"ISO-8859-1\")\n",
    "    df_sport_2022 = pd.read_csv('sports_articles_2022.csv', encoding = \"ISO-8859-1\")\n",
    "    df = pd.concat([df_sport_latest, df_sport_2022])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_2_pdset(columns, df):\n",
    "    df_vocab_select_columns = df.iloc[:, columns]\n",
    "    vocab_all_values = df_vocab_select_columns.values.ravel()\n",
    "    return set(vocab_all_values)\n",
    "\n",
    "def vocab_2_dict(sets):\n",
    "    assert(len(sets) == 4)\n",
    "    word_set = sets[0].union(sets[1],sets[2], sets[3])\n",
    "    df = pd.DataFrame(list(word_set), columns=[\"Words\"])\n",
    "    df.sort_values(by=\"Words\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return OrderedDict.fromkeys(word_set)\n",
    "\n",
    "def get_vocab_dict():\n",
    "    df_ods_vocab = pd.read_table('ods_fullforms_2020-08-26.csv', header=None)\n",
    "    df_ddo_vocab = pd.read_table('ddo_fullforms_2020-08-26.csv', header=None)\n",
    "    df_vocab = pd.read_table('cor1.02.tsv', header=None)\n",
    "    df_sport_lingo = pd.read_table('sport_lingo.csv', header=None)\n",
    "\n",
    "    vocab_set = vocab_2_pdset([1,3], df_vocab)\n",
    "    ods_vocab_set = vocab_2_pdset([0,1], df_ods_vocab)\n",
    "    ddo_vocab_set = vocab_2_pdset([0,1], df_ddo_vocab)\n",
    "    sport_lingo_set = vocab_2_pdset([0], df_sport_lingo)\n",
    "\n",
    "    return vocab_2_dict([vocab_set, ods_vocab_set, ddo_vocab_set, sport_lingo_set])\n",
    "\n",
    "ordered_dict = get_vocab_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sport = get_sports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# isin_dict = False\n",
    "def test_lookup_performance():\n",
    "    word_to_check = \"pligtsejrer\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x in range(1000000):\n",
    "        isin_dict = word_to_check in ordered_dict\n",
    "\n",
    "    end_time = time.time()  \n",
    "    assert(end_time - start_time < 1)\n",
    "    print(isin_dict)\n",
    "\n",
    "test_lookup_performance()\n",
    "\n",
    "# isin_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique words: 2739\n",
      "total sports lingo words: 978\n",
      "total vocab: 1761\n",
      "total articles: 290\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "df_sport_text = df_sport.iloc[:, [0,1,2]]\n",
    "\n",
    "sport_vocab = df_sport_text.values.ravel()\n",
    "\n",
    "words_arr = []\n",
    "\n",
    "def replace_digits(word):\n",
    "    return re.sub(r'\\d+', 'X', word)\n",
    "\n",
    "def remove_specials(word):\n",
    "    characters_to_remove = [':', \"'\", '?', \",\", \".\"]\n",
    "    new_word = word\n",
    "\n",
    "    for char in characters_to_remove:\n",
    "        new_word = new_word.replace(char, '')\n",
    "    return new_word\n",
    "\n",
    "def contains_non_alphanumeric(word):\n",
    "    return bool(re.search(r'[^a-zæøåA-ZÆØÅ0-9]', word))\n",
    "\n",
    "def formatWord(word):\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return replace_digits(word)\n",
    "    \n",
    "    \n",
    "for sentences in range(len(sport_vocab)):\n",
    "    # print(sport_vocab[sentences])\n",
    "    sentence = sport_vocab[sentences].strip()\n",
    "    words = sentence.split()\n",
    "    for word in range(len(words)):\n",
    "        w = words[word]\n",
    "        # if any(char.isdigit() for char in w):\n",
    "        #     w = replace_digits(w)\n",
    "        # if contains_non_alphanumeric(w):\n",
    "        #     w = remove_specials(w)\n",
    "        words_arr.append(w.lower())\n",
    "\n",
    "words_sport_unique = set(words_arr)\n",
    "words_sport_unique_list = list(words_sport_unique)\n",
    "words_sport_lingo = []\n",
    "words_train_vocab = []\n",
    "\n",
    "# TODO : brug tensorflow Tokenezier til at omdanne ord til tokens\n",
    "# TODO : søg i alle leksikoner, søg med og uden bindestreg\n",
    "# TODO : håndter tal ikke i ordbøger eks ( x-x eller x-årig)\n",
    "\n",
    "for w in range(len(words_sport_unique)):\n",
    "    isin_dict = words_sport_unique_list[w] in ordered_dict\n",
    "    if (isin_dict == False):\n",
    "        words_sport_lingo.append(words_sport_unique_list[w])\n",
    "    else:\n",
    "        words_train_vocab.append(words_sport_unique_list[w])\n",
    "\n",
    "print(\"total unique words:\", len(words_sport_unique) )\n",
    "print(\"total sports lingo words:\", len(words_sport_lingo) )\n",
    "print(\"total vocab:\", len(words_train_vocab))\n",
    "print(\"total articles:\", len(df_sport) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sport_lingo\n",
    "file = open('sport_lingo.txt','w')\n",
    "for item in words_sport_lingo:\n",
    "\tfile.write(item+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "file = open('sport_vocab.txt','w')\n",
    "for item in words_train_vocab:\n",
    "\tfile.write(item+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_2_bool(x):\n",
    "    if type(x) == bool:\n",
    "        return x\n",
    "    assert(type(x) == str)\n",
    "    x_copy = x\n",
    "    x_copy = x_copy.strip()\n",
    "    x_copy = x_copy.lower()\n",
    "    assert(x_copy == \"true\" or x_copy == \"false\")\n",
    "    if x_copy == \"true\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "144\n",
      "[ True False False False  True False False False False False False  True\n",
      " False  True False  True False  True False  True  True  True False False\n",
      "  True False  True False False  True  True False False  True False False\n",
      "  True False  True False False  True False  True  True False False False\n",
      " False  True False False False False  True False  True  True False  True\n",
      " False False  True False False False False False  True  True False False\n",
      "  True  True False False  True False  True  True False  True  True  True\n",
      " False  True  True  True  True False False  True False False  True False\n",
      " False  True False  True False  True  True  True False False False  True\n",
      "  True False  True False  True  True False False  True False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False  True  True False False False  True  True False False  True\n",
      "  True  True  True False  True  True  True False  True False  True False\n",
      "  True  True False  True  True  True  True  True False  True  True False\n",
      " False  True  True  True False  True False False  True  True False  True\n",
      "  True False  True  True False  True  True  True False False  True  True\n",
      "  True False False False  True False  True  True False False  True  True\n",
      "  True False  True  True False False  True False False False  True False\n",
      "  True False  True False False False  True False  True  True False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True False  True  True False False False  True  True  True  True\n",
      " False  True False False  True False  True False False  True False False\n",
      "  True False False False False False False False False False  True  True\n",
      "  True  True  True  True False  True  True False False  True False  True\n",
      "  True False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_sport_labels = df_sport['isResult'].apply(lambda x: format_2_bool(x))\n",
    "\n",
    "results_true = df_sport_labels.loc[df_sport_labels== True]\n",
    "results_false = df_sport_labels.loc[df_sport_labels == False]\n",
    "\n",
    "assert(len(results_true) + len(results_false) == len(df_sport_labels))\n",
    "\n",
    "print(len(results_true))\n",
    "print(len(results_false))\n",
    "labels = df_sport_labels.to_numpy().astype(int)\n",
    "\n",
    "print(df_sport_labels.to_numpy())\n",
    "labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HERRELANDSHOLDET  Så længe landsholdet vinder, behøver det ikke være en berusende fodboldfest  Det danske landshold ved godt, at de skal vinde, når de møder Finland i en afgørende gruppekamp i EM-kvalifikation '\n",
      " 'KVINDELANDSHOLDET  Dobbelt dansk målscorer sikrer perfekt start på ny landsholdsperiode  Danmarks fodboldlandshold har fredag aften slået Tyskland 2-0 i den første Nations League-kamp '\n",
      " 'EM HÅNDBOLD  KARAKTERER Trods storsejr får kun tre danskere høje karakterer:\\xa0Bedøm selv spillerne her  Dyk ned i karaktererne til de danske spillere efter sejren over Kroatien i den anden kamp i mellemrunden ved EM '\n",
      " 'BADMINTON  Danskers nye superserv vækker bekymring - topspillere kræver forbud  Marcus Rindshøj har nyfortolket en serv, som får Det Internationale Badmintonforbund, BWF, op af stolen '\n",
      " \"CYKLING  Vingegaard har fundet sit særlige supervåben frem:\\xa0'Han kan noget, som ingen andre kan'  Jonas Vingegaard virker igen til at have fundet topniveauet i den sidste uge af et af verdens største cykelløb \"\n",
      " 'BADMINTON  Vilde Viktor Axelsen knuser rival og er i finalen ved stor japansk turnering  Badmintonspilleren Viktor Axelsen var i hopla, da han lørdag morgen stod over for japanske Kodai Naraoka ved Japan Open '\n",
      " \"BASKETBALL  USA raser, mens Rusland sender fængslet OL-guldvinder til straffekoloni og 'slavelignende forhold'  Den amerikanske basketballspiller Brittney Griner skal afsone sin fængselsdom på ni år i en russisk straffekoloni \"\n",
      " 'EM HÅNDBOLD  Danmark må nøjes med EM-sølv efter nederlag til Norges håndboldkvinder  Danmarks håndboldlandshold taber til Norge med 25-27 i EM-finalen '\n",
      " \"CYKLING  Vingegaard i bakgear efter podiedrama:\\xa0'Jeg håber, at Sepp vinder'  Jonas Vingegaard missede førertrøjen på 17. etape med otte sekunder, som han nu håber, at Sepp Kuss beholder. \"\n",
      " 'FORMEL 1  Derfor vinder Verstappen VM:\\xa0Magnussen, mekanikeren og chefen om suveræn superstjerne  Max Verstappen er i Singapore favorit til sin 11. sejr i træk, og allerede næste weekend kan han sikre sig VM-titlen ']\n",
      "290\n",
      "290\n",
      "(290,)\n",
      "(290,)\n",
      "longest text:  312\n"
     ]
    }
   ],
   "source": [
    "df_sport_text = df_sport.iloc[:, [0,1,2]]\n",
    "\n",
    "\n",
    "df_sport_text\n",
    "df_sport_text_combined = df_sport_text.apply(' '.join, axis=1)\n",
    "train_text = df_sport_text.apply(' '.join, axis=1).to_numpy()\n",
    "\n",
    "\n",
    "print (train_text[280:290])\n",
    "# df_sport_text_combined.to_numpy().shape\n",
    "\n",
    "df_sport_labels[280:290]\n",
    "\n",
    "df_sport_text[280:290]\n",
    "print(len(labels))\n",
    "print(len(train_text))\n",
    "print(labels.shape)\n",
    "print(train_text.shape)\n",
    "print(\"longest text: \", len(max(train_text, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1424    1  133 1163 1095 1047  730  757    1  241 1716 1723 1495 1100\n",
      "   809  607 1527 1085 1252  798  647 1065 1402  791    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]], shape=(1, 400), dtype=int64)\n",
      "1740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['em', '[UNK]', 'usynlige', 'helte', 'hyldes', 'inden',\n",
       "       'nabobraget', 'mod', '[UNK]', 'tirsdag', 'aften', 'afgøres', 'det',\n",
       "       'hvor', 'mange', 'point', 'danmarks', 'håndboldkvinder', 'får',\n",
       "       'med', 'over', 'i', 'em-turneringens', 'mellemrunde', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "def replace_digits(word):\n",
    "    return tf.strings.regex_replace(word, pattern=r'\\d+', rewrite=r'X')\n",
    "\n",
    "def remove_specials(word):\n",
    "    return tf.strings.regex_replace(word, pattern=r'[:,\\'\\.]', rewrite=r'')\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return remove_specials(replace_digits(lowercase))\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 2000\n",
    "embedding_dim = 128\n",
    "sequence_length = 400\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "text_ds = vectorize_layer.adapt(words_train_vocab)\n",
    "\n",
    "vect_vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "text_vec = vectorize_layer([train_text[5]])\n",
    "\n",
    "def vect_layer_2_text(vect_l):\n",
    "    return np.array([vect_vocab[x] for x in np.squeeze(vect_l.numpy())])\n",
    "\n",
    "\n",
    "# text = vect_layer_2_text(text_vec)\n",
    "\n",
    "print(text_vec)\n",
    "print(len(vect_vocab))\n",
    "vect_layer_2_text(text_vec)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(90, 400), dtype=int64, numpy=\n",
       "array([[1654,  101, 1445, ...,    0,    0,    0],\n",
       "       [1654,    1,   83, ...,    0,    0,    0],\n",
       "       [1654,    1,    1, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1424,    1,  904, ...,    0,    0,    0],\n",
       "       [1424,    1,  133, ...,    0,    0,    0],\n",
       "       [   1,  903,  281, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_ds = vectorize_layer(train_text)\n",
    "\n",
    "train_data = train_ds[0:230]\n",
    "val_data = train_ds[230:]\n",
    "\n",
    "train_labels = labels[0:230]\n",
    "val_labels = labels[230:]\n",
    "\n",
    "train_ds\n",
    "val\n",
    "# labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 32, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 32, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7130 - accuracy: 0.4870 - val_loss: 0.6936 - val_accuracy: 0.4667\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.6781 - accuracy: 0.6304 - val_loss: 0.6644 - val_accuracy: 0.5500\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.3891 - accuracy: 0.8217 - val_loss: 0.8856 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1837 - accuracy: 0.9435 - val_loss: 0.5301 - val_accuracy: 0.7833\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0824 - accuracy: 0.9826 - val_loss: 0.5729 - val_accuracy: 0.7833\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0782 - accuracy: 0.9870 - val_loss: 0.5798 - val_accuracy: 0.7833\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0836 - accuracy: 0.9870 - val_loss: 0.5883 - val_accuracy: 0.8000\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0845 - accuracy: 0.9870 - val_loss: 0.5798 - val_accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0672 - accuracy: 0.9870 - val_loss: 0.6314 - val_accuracy: 0.7833\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0616 - accuracy: 0.9870 - val_loss: 0.6111 - val_accuracy: 0.8167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b8215f9a0>"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_data, train_labels, epochs=epochs, batch_size=8, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4993991255760193\n",
      "Test accuracy: 0.8833333253860474\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(val_data, val_labels, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99]\n",
      " [0.  ]\n",
      " [0.23]\n",
      " [1.  ]\n",
      " [0.98]\n",
      " [0.02]\n",
      " [0.  ]\n",
      " [0.  ]\n",
      " [1.  ]\n",
      " [1.  ]\n",
      " [1.  ]\n",
      " [1.  ]\n",
      " [0.12]\n",
      " [0.73]\n",
      " [0.  ]\n",
      " [0.88]\n",
      " [1.  ]\n",
      " [0.  ]\n",
      " [1.  ]\n",
      " [0.99]]\n",
      "labels:\n",
      "[1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision = 2, suppress = True)\n",
    "predictions = model.predict(val_data[10:30])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(predictions)\n",
    "print(\"labels:\")\n",
    "print(val_labels[10:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array values with precision 2:\n",
      "\n",
      "[  0.     1.59 150.45   0.29]\n"
     ]
    }
   ],
   "source": [
    "num = np.array([1.8e-10, 1.586, 150.45, 0.2855])\n",
    " \n",
    "# Suppressing 1-D numpy array with precision 2\n",
    "# using numpy.set_printoptions()\n",
    "print(\"Numpy array values with precision 2:\\n\")\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['badminton' 'vi' 'er' 'blandt' 'de' 'bedste' 'i' '[UNK]' 'trofæ' 'giver'\n",
      " 'badmintonstjerner' 'tro' 'på' 'ol-succes' 'weekendens' 'sejr' 'i'\n",
      " '[UNK]' 'er' 'endnu' 'et' 'bevis' 'på' 'at' 'herredoublen' 'kim' '[UNK]'\n",
      " 'og' '[UNK]' '[UNK]' '[UNK]' 'er' 'i' 'storform' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vect_layer_2_text(val_data[11]))\n",
    "val_labels[11]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be0503bf1d8a1ee3ca0077be831d95fbcddd9686f11808f41fa1809452b7e6ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('newenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
