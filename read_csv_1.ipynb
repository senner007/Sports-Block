{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import OrderedDict\n",
    "def get_sports():\n",
    "    df_sport_latest = pd.read_csv('articles/sports_articles.csv', encoding = \"ISO-8859-1\")\n",
    "    df_sport_latest_tv2 = pd.read_csv('articles/sports_articles_tv2.csv', encoding = \"ISO-8859-1\")\n",
    "    df_sport_2019 = pd.read_csv('articles/sports_articles_2019.csv', encoding = \"ISO-8859-1\")\n",
    "    df_sport_2020 = pd.read_csv('articles/sports_articles_2020.csv', encoding = \"ISO-8859-1\")\n",
    "    df_sport_2022 = pd.read_csv('articles/sports_articles_2022.csv', encoding = \"ISO-8859-1\")\n",
    "    df = pd.concat([df_sport_latest, df_sport_latest_tv2, df_sport_2019, df_sport_2020, df_sport_2022])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_2_pdset(columns, df):\n",
    "    df_vocab_select_columns = df.iloc[:, columns]\n",
    "    vocab_all_values = df_vocab_select_columns.values.ravel()\n",
    "    return set(vocab_all_values)\n",
    "\n",
    "def vocab_2_dict(sets):\n",
    "    assert(len(sets) == 4)\n",
    "    word_set = sets[0].union(sets[1],sets[2], sets[3])\n",
    "    df = pd.DataFrame(list(word_set), columns=[\"Words\"])\n",
    "    df.sort_values(by=\"Words\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return OrderedDict.fromkeys(word_set)\n",
    "\n",
    "def get_vocab_dict():\n",
    "    df_ods_vocab = pd.read_table('ods_fullforms_2020-08-26.csv', header=None)\n",
    "    df_ddo_vocab = pd.read_table('ddo_fullforms_2020-08-26.csv', header=None)\n",
    "    df_vocab = pd.read_table('cor1.02.tsv', header=None)\n",
    "    df_sport_lingo = pd.read_table('sport_lingo.csv', header=None)\n",
    "\n",
    "    vocab_set = vocab_2_pdset([1,3], df_vocab)\n",
    "    ods_vocab_set = vocab_2_pdset([0,1], df_ods_vocab)\n",
    "    ddo_vocab_set = vocab_2_pdset([0,1], df_ddo_vocab)\n",
    "    sport_lingo_set = vocab_2_pdset([0], df_sport_lingo)\n",
    "\n",
    "    return vocab_2_dict([vocab_set, ods_vocab_set, ddo_vocab_set, sport_lingo_set])\n",
    "\n",
    "ordered_dict = get_vocab_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sport = get_sports()\n",
    "\n",
    "df_sport_combined = df_sport.copy().drop('Link', axis=1)\n",
    "df_sport_combined.to_csv('articles_temp/combined.csv')\n",
    "\n",
    "\n",
    "duplicate_rows = df_sport.duplicated()\n",
    "\n",
    "print(df_sport[duplicate_rows])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sport_combined.iloc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# isin_dict = False\n",
    "def test_lookup_performance():\n",
    "    word_to_check = \"linebreak\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x in range(1000000):\n",
    "        isin_dict = word_to_check in ordered_dict\n",
    "\n",
    "    end_time = time.time()  \n",
    "    assert(end_time - start_time < 1)\n",
    "    print(isin_dict)\n",
    "\n",
    "test_lookup_performance()\n",
    "\n",
    "# isin_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train_text = df_sport.iloc[:, [0,1,2]].apply(' '.join, axis=1).replace('\\xa0', '', regex=True).to_numpy()\n",
    "\n",
    "words_arr = []\n",
    "max_words_insentence = 0\n",
    "\n",
    "def replace_digits(word):\n",
    "    return re.sub(r'\\d+', 'X', word)\n",
    "\n",
    "def remove_specials(word):\n",
    "    words_new = []\n",
    "    parts = re.findall(r\"[A-ZÆØÅa-zæøå0-9]+|\\S\", word)\n",
    "    words_new.extend([x for x in parts])\n",
    "    return words_new\n",
    "\n",
    "def contains_non_alphanumeric(word):\n",
    "    return bool(re.search(r'[^a-zæøåA-ZÆØÅ0-9]', word))\n",
    "\n",
    "def formatWord(word):\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return replace_digits(word)\n",
    "    \n",
    "    \n",
    "for ind, sentence in enumerate(train_text):\n",
    "    sentence_trimmed = sentence.strip()\n",
    "    words = sentence_trimmed.split()\n",
    "    current_count = len(words_arr)\n",
    "    for word in words:\n",
    "        w = remove_specials(word)\n",
    "        words_arr.extend([x.lower() for x in w])\n",
    "\n",
    "    after_count = len(words_arr) - current_count\n",
    "    if after_count > max_words_insentence:\n",
    "        max_words_insentence = after_count\n",
    "\n",
    "\n",
    "words_sport_unique = set(words_arr)\n",
    "words_sport_unique_list = list(words_sport_unique)\n",
    "words_sport_lingo = []\n",
    "words_train_vocab = []\n",
    "\n",
    "# TODO : brug tensorflow Tokenezier til at omdanne ord til tokens\n",
    "# TODO : søg i alle leksikoner, søg med og uden bindestreg\n",
    "# TODO : håndter tal ikke i ordbøger eks ( x-x eller x-årig)\n",
    "# TODO : lemmatizer : udelad bøjninger af samme navneord. eks : verdensmester/verdensmesteren\n",
    "# TODO : evt. grupper ord der ofte hænger sammen med nltk BigramFinder. eks vandt over\n",
    "\n",
    "for w in range(len(words_sport_unique_list)):\n",
    "    word = words_sport_unique_list[w]\n",
    "    if any(char.isdigit() for char in word):\n",
    "        words_train_vocab.append(word)\n",
    "    if contains_non_alphanumeric(word):\n",
    "        words_train_vocab.append(word)\n",
    "    else: \n",
    "        isin_dict = word in ordered_dict\n",
    "        if (isin_dict == False):\n",
    "            words_sport_lingo.append(word)\n",
    "        else:\n",
    "            words_train_vocab.append(word)\n",
    "\n",
    "\n",
    "print(\"Max words in sentence: \", max_words_insentence)\n",
    "print(\"total unique words:\", len(words_sport_unique) )\n",
    "print(\"total sports lingo words:\", len(words_sport_lingo) )\n",
    "print(\"total vocab:\", len(words_train_vocab))\n",
    "print(\"total articles:\", len(df_sport) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopwords(word):\n",
    "#     if word in da_stopwords:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# words_train_vocab = [x for x in words_train_vocab if not remove_stopwords(x)]\n",
    "\n",
    "# words_train_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# cnt = Counter()\n",
    "# for word in words_train_vocab:\n",
    "#    cnt[word] += 1\n",
    "\n",
    "# s = sorted(cnt.items(), key=lambda item: item[1])\n",
    "# s.reverse()\n",
    "\n",
    "# s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sport_lingo\n",
    "file = open('words_sport_lingo.txt','w')\n",
    "for item in words_sport_lingo:\n",
    "\tfile.write(item+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "file = open('words_train_vocab.txt','w')\n",
    "for item in sorted(words_train_vocab):\n",
    "\tfile.write(item+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_2_bool(x):\n",
    "    if type(x) == bool:\n",
    "        return x\n",
    "    assert(type(x) == str)\n",
    "    x_copy = x\n",
    "    x_copy = x_copy.strip()\n",
    "    x_copy = x_copy.lower()\n",
    "    assert(x_copy == \"true\" or x_copy == \"false\")\n",
    "    if x_copy == \"true\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sport_labels = df_sport['isResult'].apply(lambda x: format_2_bool(x))\n",
    "\n",
    "results_true = df_sport_labels.loc[df_sport_labels== True]\n",
    "results_false = df_sport_labels.loc[df_sport_labels == False]\n",
    "\n",
    "assert(len(results_true) + len(results_false) == len(df_sport_labels))\n",
    "\n",
    "print(len(results_true))\n",
    "print(len(results_false))\n",
    "labels = df_sport_labels.to_numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(train_text))\n",
    "print(labels.shape)\n",
    "print(train_text.shape)\n",
    "longest_text = len(max(train_text, key=len))\n",
    "print(\"longest text: \", longest_text)\n",
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "# TODO : evt indikere hvilke navneord der starte med stort bogstav(egenavne), evt. lave et opslag for at undersøge ordklasse for det første ord i sætningen \n",
    "# TODO : adskil og inkluder punktuering (,.\"\"?) \n",
    "\n",
    "def to_lower(word):\n",
    "    return tf.strings.lower(word, encoding='utf-8')\n",
    "\n",
    "def split_specials(input_data):\n",
    "    fd = [\":\" , \"/\", \":\", \",\", \"'\", \".\", \"?\", \"-\", \"!\", \"(\", \")\", '\"']\n",
    "\n",
    "    new_str = input_data\n",
    "    for sign in fd:\n",
    "        r = \"\\\\\" + sign\n",
    "        new_str = tf.strings.regex_replace(new_str, pattern=r, rewrite=\" \" + sign + \" \")\n",
    "\n",
    "    return new_str\n",
    "\n",
    "def replace_digits(word):\n",
    "    return tf.strings.regex_replace(word, pattern=r'\\d+', rewrite=r'X')\n",
    "\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = to_lower(input_data)\n",
    "    replaced_digits = replace_digits(lowercase)\n",
    "    return split_specials(replaced_digits)\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 3300\n",
    "sequence_length = 100\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "\n",
    "text_ds = vectorize_layer.adapt(words_train_vocab)\n",
    "vect_vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "def vect_layer_2_text(vect_l):\n",
    "    return np.array([vect_vocab[x] for x in np.trim_zeros(np.squeeze(vect_l.numpy()))])\n",
    "\n",
    "print(\"Total vocab/max_features : \",  len(vect_vocab))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_text[0:10]:\n",
    "    print(\"Original \\n:\", t)\n",
    "    print(\"vect_2_text: \\n\", vect_layer_2_text(vectorize_layer([t])))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, percentage):\n",
    "    l = len(data)\n",
    "    p = l - int((percentage/100) * l)\n",
    "    return (data[0:p], data[p:], labels[0:p], labels[p:])\n",
    "\n",
    "\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = split_data(vectorize_layer(train_text), labels, 10)\n",
    "\n",
    "print(\"Total data points\", len(train_text))\n",
    "print(\"Train data length\", len(train_data))\n",
    "print(\"Valkidation data length\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import random as python_random\n",
    "\n",
    "\n",
    "\n",
    "def get_transformer_model():\n",
    "\n",
    "    embed_dim = 96  # Embedding size for each token\n",
    "    num_heads = 3  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "    # A integer input for vocab indices.\n",
    "    inputs = tf.keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "    # 'embedding_dim'.\n",
    "    # x = layers.Embedding(max_features, embed_dim)(inputs)\n",
    "\n",
    "    embedding_layer = TokenAndPositionEmbedding(sequence_length, max_features, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    # x = layers.Conv1D(128, 10, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    # x = layers.Conv1D(128, 10, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "\n",
    "    transformer_model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    transformer_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return transformer_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import random as python_random\n",
    "\n",
    "def get_cnn_model():\n",
    "\n",
    "    embedding_dim = 96\n",
    "\n",
    "    # A integer input for vocab indices.\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "    # 'embedding_dim'.\n",
    "    x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(128, 10, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = layers.Conv1D(128, 10, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "    cnn_model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return cnn_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_model(name):\n",
    "    if (name == \"cnn\"):\n",
    "       return get_cnn_model()\n",
    "    elif (name == \"transformer\"):\n",
    "       return get_transformer_model()\n",
    "  \n",
    "\n",
    "def filter_max_accuracy(history, threshold = 0.99):\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    list = []\n",
    "    for x in range(len(acc)):\n",
    "        if (acc[x] > threshold):\n",
    "            list.append(val_acc[x])\n",
    "\n",
    "    return np.array(list)\n",
    "\n",
    "models = [\"cnn\", \"transformer\"]\n",
    "\n",
    "\n",
    "def mean_model_accuracy(mode_names, iterations, epochs = 20):\n",
    "\n",
    "    callback_3_loss = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name in range(len(mode_names)):\n",
    "        model_name = mode_names[name]\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for x in range(iterations):\n",
    "            model = prepare_model(model_name)\n",
    "\n",
    "            # Fit the model using the train and test datasets.\n",
    "            history = model.fit(train_data, train_labels, epochs=epochs, batch_size=6, validation_data=(val_data, val_labels), callbacks=[callback_3_loss])\n",
    "\n",
    "            max_val_acc = filter_max_accuracy(history)\n",
    "            val_accuracies.append(max(max_val_acc))\n",
    "            print(max(max_val_acc))\n",
    "            print(val_accuracies)\n",
    "        \n",
    "        d = dict(name = model_name, results = np.mean(np.squeeze(np.array(val_accuracies))))\n",
    "        results.append(d)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results = mean_model_accuracy(models, 10)\n",
    "mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_score(model):\n",
    "    score = model.evaluate(val_data, val_labels, verbose=0)\n",
    "    print(\"Validation loss:\", score[0])\n",
    "    print(\"Validations accuracy:\", score[1])\n",
    "\n",
    "def print_validation_results(predictions):\n",
    "    for x in range(len(val_data)):\n",
    "        print(\"VALIDATION SAMPLE: \\n\" ,vect_layer_2_text(val_data[x]))\n",
    "        print(\"LABEL --:\" , val_labels[x])\n",
    "        print(\"PREDICTION --:\" , predictions[x])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs= 5\n",
    "transformer_model = get_transformer_model()\n",
    "\n",
    "# # Fit the model using the train and test datasets.\n",
    "transformer_history = transformer_model.fit(train_data, train_labels, epochs=epochs, batch_size=6, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs= 5\n",
    "cnn_model = get_cnn_model()\n",
    "\n",
    "# # Fit the model using the train and test datasets.\n",
    "transformer_history = cnn_model.fit(train_data, train_labels, epochs=epochs, batch_size=6, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model):\n",
    "    np.set_printoptions(precision = 5, suppress = True)\n",
    "    predictions = model.predict(val_data)\n",
    "    print_model_score(model)\n",
    "    print(\"\\n\")\n",
    "    print_validation_results(predictions)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- TRANSFORMER ---\")\n",
    "print_results(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--- CNN ---\")\n",
    "print_results(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "\n",
    "ll = transformer_model.layers[1]\n",
    "ll_weights = ll.get_weights()[0]\n",
    "\n",
    "print(ll_weights.shape)\n",
    "ll_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import I/O module in python\n",
    "import io\n",
    "\n",
    "##open the text stream for vectors\n",
    "vectors = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "##open the text stream for metadata\n",
    "meta = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "##write each word and its corresponding embedding\n",
    "for index in range(1, len(vect_vocab)):\n",
    "  word = vect_vocab[index]  # flipping the key-value in word_index\n",
    "  embeddings = ll_weights[index]\n",
    "  meta.write(word + \"\\n\")\n",
    "  vectors.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "\n",
    "##close the stream\n",
    "vectors.close()\n",
    "meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import collocations\n",
    "# bigram_measures = collocations.BigramAssocMeasures()\n",
    "# finder = collocations.BigramCollocationFinder.from_words([\"New\", \"York\", \"is\", \"big\", \"New\", \"York\", \"is\", \"dirty\"])\n",
    "# finder.ngram_fd.items()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lemmy\n",
    "# # Create an instance of the standalone lemmatizer.\n",
    "# lemmatizer = lemmy.load(\"da\")\n",
    "\n",
    "# # Find lemma for the word 'akvariernes'. First argument is an empty POS tag.\n",
    "# lemmatizer.lemmatize(\"NOUN\", \"storsejr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk as nltk\n",
    "# # from string import punctuation\n",
    "# # from nltk.corpus import stopwords\n",
    "# # nltk.download('stopwords')\n",
    "\n",
    "# # da_stopwords = stopwords.words(\"danish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = transformer_model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"transformer_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSkisport, Results: \")\n",
    "print( end_to_end_model.predict(\n",
    "    [\n",
    "    \"Skisport Buller overrasker alle og gør det umulige. Ny rekord sikrer flerer medalje til Norge\",\n",
    "    \"Skisport Buller overrasker alle. Hollænderen havde masser af overskud, da han kom i mål som nummer to\",\n",
    "     ]))\n",
    "\n",
    "print(\"\\nSkisport, other: \")\n",
    "\n",
    "print( end_to_end_model.predict(\n",
    "    [\n",
    "    \"Skisport Sverige drømmer om flere medaljer og sejre til næste års OL. Træner har et godt øje til hidtil ukendt talent\",\n",
    "    \"Skisport Flere forventer podiepladser til Norge ved kommende VM. Træner har et godt øje til hidtil ukendt talent\",\n",
    "    \"Skisport Denne gang er det alvor. Buller: 'Jeg har satset alt'\",\n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nFodbold, Results:\")\n",
    "\n",
    "print(end_to_end_model.predict(\n",
    "    [\n",
    "      \"Fodbold Fjerritslev vinder over Vordingborg. Træner kommenterer på historisk kamp\",\n",
    "\n",
    "     ]))\n",
    "\n",
    "print(\"\\nFodbold, other:\") \n",
    "print(end_to_end_model.predict(\n",
    "    [\n",
    "      \"Fodbold Træner forventer at Fjerritslev vinder over Vordingborg. 'Det bliver en historisk kamp'\",\n",
    "     ]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be0503bf1d8a1ee3ca0077be831d95fbcddd9686f11808f41fa1809452b7e6ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('newenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
